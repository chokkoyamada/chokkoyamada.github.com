<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: aws | Continuous Ops]]></title>
  <link href="http://chokkoyamada.github.io/blog/categories/aws/atom.xml" rel="self"/>
  <link href="http://chokkoyamada.github.io/"/>
  <updated>2014-07-18T16:57:59+09:00</updated>
  <id>http://chokkoyamada.github.io/</id>
  <author>
    <name><![CDATA[Naoyuki Yamada]]></name>
    <email><![CDATA[yamada_naoyuki@cyberagent.co.jp]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AWS Summit Tokyo 2014: Chatwork検索機能を支える技術]]></title>
    <link href="http://chokkoyamada.github.io/blog/2014/07/18/aws-summit-tokyo-2014-chatwork/"/>
    <updated>2014-07-18T16:11:00+09:00</updated>
    <id>http://chokkoyamada.github.io/blog/2014/07/18/aws-summit-tokyo-2014-chatwork</id>
    <content type="html"><![CDATA[<p>ChatworkがAWSで作られているのは周知の事実でしたが、これだけのものをどう構築しているのかはとても気になっていました。</p>

<p>ガンホーのセッションとどちらにいくかを迷いましたが、日頃お世話になっているChatworkのほうを聞きにきました。</p>

<p>オープンソースの検索ミドルウェアがいくつか選択肢があり、特別なノウハウが無くても検索システムを作ることは可能な時代になってきました。ただしその運用は簡単じゃなく、大量データの投入, 検索速度, バックアップなど課題が多いと思っています。</p>

<p>私も大規模なElasticSearchクラスタを運用していますが、このセッションでもあった通り、心が折れそうになる気持ちは分かります。
その道のスペシャリストになれればよいのですが、それだけの仕事をしているわけではないので・・・中途半端に手を出すとやけどしますね。</p>

<p>そんななかでChatworkがあれだけの規模をなんとか動かし続けているのはCloudsearchのおかげな部分が大きいということです。
やはり少人数で回すには、フルマネージドサービスさま様ですね。。。</p>

<p>Chatowkの規模：4万6千社、41万ユーザー</p>

<h2>Chatwork検索システムの変遷</h2>

<ul>
<li>Mroonga単体構成 2011/9~</li>
<li>Mroonga分割構成 2013/5~</li>
<li>Elasticsearch検討 2013/6~</li>
<li>CloudSearch検討 2014/4~</li>
</ul>


<h2>CloudSearchを採用した理由</h2>

<ul>
<li><p>Mroonga 3.0の課題</p>

<ul>
<li>IOロックの発生</li>
<li>一千万件程度では順調</li>
<li>数千万件レベルになるとMySQLがダウンするように。。。</li>
</ul>
</li>
<li><p>IOロックを回避するために
Mroonga + Spiderストレージエンジンを検討
数億のメッセージ規模で安定稼働させるには、最初から多数のMroongaノードを用意する必要あり</p></li>
<li><p>Elasticsearch 0.9の検証
構成する要素が複雑: クラスター、ノード、インデックス、シャード
サイジングが必要
インデックス作成時にシャード数を決める
シャード数は後から変更できない</p></li>
<li><p>ダミーメッセージ投入テストを行いながらTry and Error</p></li>
<li><p>シャード分割数が少なすぎ、書き込み速度が徐々に低下、シャード分割数を増やしてデータ再投入</p></li>
<li><p>サイジング対策
インデックスを範囲で区切って細かく分割, シャードも細かく、クラスタ構成も大きなもので</p></li>
<li><p>i2.xlarge x 6の構成を予定</p></li>
<li><p>課題</p>

<ul>
<li>バックアップｘ</li>
<li>Multi-az x</li>
<li>アクセス制御 x</li>
</ul>
</li>
</ul>


<p>もっとカジュアルに検索システムを開発・運用したい。</p>

<ul>
<li>2014/3にCloudsearch大幅アップデートで検証開始

<ul>
<li>初期投入が十分早い</li>
<li>検索速度も早い</li>
<li>Multi-AZ運用が可能</li>
<li>Index Fieldごとにアクセス制御可能</li>
</ul>
</li>
</ul>


<h2>CloudSearchを利用した検索アーキテクチャ</h2>

<p>キューを通してワーカーがDynamoDBに履歴を作って、それをまとめてCloudsearchに入れる構成</p>

<h3>機能要件</h3>

<ul>
<li>複数言語対応</li>
<li>閲覧権限を持つ全てのGroupChat</li>
<li>初期投入対象は3.2億件</li>
<li><p>差分投入で追加、編集、削除を順次行う</p></li>
<li><p>Domain設計</p>

<ul>
<li>Scaling Options:

<ul>
<li>search.m2.2xlarge</li>
<li>partition count: 4</li>
</ul>
</li>
<li>Index Option

<ul>
<li>(Multiple Language: 特定の言語に依存しない index fieldを一つ用意)</li>
<li>言語ごとにIndex fieldを作成、言語判定ライブラリを用意</li>
</ul>
</li>
</ul>
</li>
<li><p>3.2億件の初期データ投入結果</p>

<ul>
<li>SQSで分割 documents/batch requestを利用</li>
<li>c3.8xlargeをworker</li>
<li>30並列で実行</li>
<li>12時間</li>
<li>データ投入自体にかかったコストは450ドルくらい</li>
</ul>
</li>
<li><p>差分投入</p>

<ul>
<li>チャットメッセージ編集・追加・削除ごとにSQS作成</li>
<li>一定以上たまったらworkerがまとめて投入</li>
<li>この構成で大きな遅延なくindexできている</li>
</ul>
</li>
</ul>


<h2>CloudSearchの課題</h2>

<ul>
<li><p>検索</p>

<ul>
<li>記号の検索に対応していない</li>
<li>アプリケーション側で工夫が必要</li>
</ul>
</li>
<li><p>Cloudwatchのメトリクスがない</p>

<ul>
<li>代替手段として、describe</li>
<li>SQSの残キュー数の取得</li>
<li>muninを使用</li>
</ul>
</li>
<li><p>認証</p>

<ul>
<li>Scaleoutするためには、現状Proxyが必要</li>
<li>access policyはグローバルIPが指定可能だが、即時反映されない、実質４０分程度かかる</li>
</ul>
</li>
<li><p>コスト</p>

<ul>
<li>実質的な運用コストはMroonga分割運用時と比較して約２倍になった</li>
<li>リザーブドインスタンスがない</li>
</ul>
</li>
</ul>


<h2>採用してよかったこと</h2>

<ul>
<li>保守運用が楽になった</li>
<li>差分投入が安定</li>
<li>検索速度の安定、高速</li>
<li>アプリケーション構成がシンプル</li>
<li>AWSサポートから日本語でサポートを受けられる</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS Summit Tokyo 2014: Amazon Kinesis Deep Dive]]></title>
    <link href="http://chokkoyamada.github.io/blog/2014/07/18/aws-summit-tokyo-2014-kinesis-deep-dive/"/>
    <updated>2014-07-18T15:10:00+09:00</updated>
    <id>http://chokkoyamada.github.io/blog/2014/07/18/aws-summit-tokyo-2014-kinesis-deep-dive</id>
    <content type="html"><![CDATA[<p>ソリューションアーキテクトの大谷さんと開発マネージャの堀さんによる、Kinesisの詳説です。
Kinesisの開発マネージャの方が解説してくれたこともあり、とても意義あるものでした。</p>

<h3>Kinesisの新規ローンチ</h3>

<ul>
<li>東京リージョンローンチ</li>
<li>Fluentdプラグイン <a href="https://github.com/awslabs/aws-fluent-plugin-kinesis">https://github.com/awslabs/aws-fluent-plugin-kinesis</a></li>
<li>MQTTアダプター <a href="https://github.com/awslabs/mqtt-kinesis-bridge">https://github.com/awslabs/mqtt-kinesis-bridge</a></li>
</ul>


<h2>事例紹介</h2>

<ul>
<li>ガリバー <a href="http://221616.com/gulliver/news/press/20140715-13667.html">http://221616.com/gulliver/news/press/20140715-13667.html</a></li>
<li>Pencil(リアルタイムユーザーモニター)</li>
<li>SmartInsight(Beaconとの距離をロギング)</li>
<li>Supercell</li>
<li>bizo</li>
</ul>


<h2>Kinesis詳細</h2>

<h3>なぜリアルタイム？</h3>

<ul>
<li>測定(Metering)サービス　使った分だけ課金されるという特性　オペレーションを測定しないといけない</li>
<li>毎秒数千万レコード</li>
<li>毎時数テラバイト</li>
<li>数十万のデータ・ソース</li>
<li><p>月末にはオーディターでの100%の正確性</p></li>
<li><p>毎時数百万のファイル</p></li>
<li>スケールの課題</li>
<li>リアルタイムの要望</li>
<li><p>高い運用コスト</p></li>
<li><p>要求の変化
毎時か毎日のデータ処理が従来の要求だったが、リアルタイム、早い意思決定、KeepEverything、エラスティックな拡張性、複数の目的に応じて同じデータを並行処理したい、などの新しい要求が出てきた。</p></li>
</ul>


<h3>Kinesis概要</h3>

<p>用途単位でStreamを作成し、Streamは１つ以上のShardで構成される
Shardは入力側秒間１MB 1000TPB 出力側2mB, ５TPSのキャパシティ
入力されたデータは複数のAZに２４時間保存
Shardの増減でスケール</p>

<h3>データ入力</h3>

<p>HTTPS/POSt
SDK
Fluentd
Flume
Log4J
etc</p>

<p>ProducerがPut Recordするサンプル AWS CLIでできる</p>

<p>Shardへの分配ロジック：　md5でハッシュ化して該当のShardに分配される
パーティションキーを何にするかは重要</p>

<p>パーティションキーの数 > shardの数
カーディナリティーの高いパーティションキーを使う</p>

<p>シーケンス番号を使って何度でも読むことができる
何度取得してもシーケンス番号の順番は変わらない:重要</p>

<p>データ取得と処理</p>

<p>API
Kinesis Client Library/ConnectorLibrary
Storm
EMR</p>

<h2>どうやって信頼性と拡張性のあるアプリケーションを作るの？</h2>

<ul>
<li>Kinesis Client Libraryを提供

<ul>
<li>Shardと同じ数のWorker</li>
<li>Workerを均等にロードバランシング</li>
<li>障害完治と新しいWorkerの立ち上げ</li>
<li>shardの数に応じてWorkerが動作する</li>
<li>Autoscaling</li>
<li>チェックポインティング、At least once処理</li>
</ul>
</li>
</ul>


<p>これらの煩雑な処理を意識することなく、ビジネスロジックに集中できる</p>

<h3>重複データについて</h3>

<p>ネットワーク障害や500レベルエラーはリトライをする
リカバリやロードバランシングでデータが最後のチェックポイントからリプレイされる</p>

<ul>
<li>ユースケースを理解する

<ul>
<li>冪等なアプリをつくる</li>
<li>重複を許容する</li>
</ul>
</li>
</ul>


<h3>Kinesis Connector Library</h3>

<p>S3, DynamoDB, Redshift
4つのインタフェースを使うと簡単に書ける</p>

<p>1つのデータをいろいろなシステムで
プロデューサーの負荷軽減
データの一貫性を保ちたい</p>

<p>Kinesisに全てのデータを１回入力する
必要に応じて新しいアプリを追加していく
Agilityを高める</p>

<h2>Elasticな拡張性</h2>

<ul>
<li>Streamは１つ以上のキャパシティで構成　Shardでキャパシティプランニング</li>
<li>SplitShard API Shardを半分に分割</li>
</ul>


<p>shardやEC2もキャパシティがある
ProvisionedThroughputExceededException</p>

<p>shard１つで14$/month
Getトランザクションは無料</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS Summit Tokyo 2014: Amazon Kinesisを用いたリアルタイムのゲーム分析システムと、AWS ElasticBeanstalk＆Amazon DynamoDB を活用したゲーム運営]]></title>
    <link href="http://chokkoyamada.github.io/blog/2014/07/18/aws-summit-tokyo-2014-dynamo-kinesis/"/>
    <updated>2014-07-18T14:15:00+09:00</updated>
    <id>http://chokkoyamada.github.io/blog/2014/07/18/aws-summit-tokyo-2014-dynamo-kinesis</id>
    <content type="html"><![CDATA[<p>Ripplationという会社の安藤さんと立本さんによるゲーム運営にAWSのマネージドサービスを活用する話です。
最近のゲーム界隈にうといのですが、「騎士とドラゴン」が代表的タイトルですかね。</p>

<p>とことん運用を楽にする側に振り切っているのは清々とします。</p>

<p>Kinesis &ndash; ElasticBeansTalk &ndash; S3の組み合わせ方がかなり独特で、二段階に処理を分けるというのはとてもおもしろいと思いました</p>

<ul>
<li>いかにして最小人数でサーバー側の開発を行うか</li>
<li><p>以下にしてサーバーコストを下げるか</p></li>
<li><p>スケール・耐障害の自動化</p></li>
<li>監視システムも簡単にしたい</li>
</ul>


<p>なので、ElasticBeansTalk + DynamoDBのみで構成することにした</p>

<h2>AWSを使ってみてわかったこと</h2>

<ul>
<li>すぐ始められる</li>
<li>アプリをサーバーにデプロイするまでに流れを作るのが簡単</li>
<li>1から整える必要がない</li>
<li>思った以上に実用に耐えられる</li>
<li>ほとんど放置が可能: 最近サーバーをコンソールで調整したくらい</li>
</ul>


<h3>ElasticBeansTalkの注意点</h3>

<ul>
<li>デプロイ時に一瞬アクセスできないことがある: 気にせずデプロイしている。エラー処理はアプリケーション側で行っている</li>
<li>オートスケーリングの調整はきっちりしておく</li>
</ul>


<h3>DynamoDBの注意点</h3>

<ul>
<li>社内にいるDB技術者の文句やいちゃもんに負けない　問題が出たら後付けで</li>
<li>バッチ処理をしっかり使う、リトライも行う</li>
<li>スループットの限界値は急激に変化させられない: 急激な書き込みは読み込みはできない</li>
<li>バックアップするかどうか　リストアする場合ってどんな状況だろうか</li>
</ul>


<h3>汎用化なんて考えたらダメ</h3>

<ul>
<li>AWSを使い倒さないと真の良さは発揮されない</li>
<li>そんなにどれでも動く仕組みって重要なんでしょうか。</li>
</ul>


<h2>ログデータの収集・集計</h2>

<ul>
<li>結局、最初はDynamoDBだけでなんとかした</li>
</ul>


<h2>KinesisとBigData</h2>

<h3>ビッグデータとはなにか</h3>

<p>単体ではよくわからないデータこそがビッグデータ</p>

<p>達成課題：毎秒10万件のデータを収集する</p>

<p>Linux
TCP
28000個ぐらいのポート
TIMEWAIT 60sec
実測値への縫製係数: 0.6
1server 282 req/sec</p>

<p>チューニング後：</p>

<p>Linux
TCP
64000 Port
TIMEWAIT 10sec
1server 4000req/sec</p>

<p>26台あれば　 3870x26=100620req/sec</p>

<p>ただ、これをどうやって集める？ 一般的なサーバーでつくろうとするとdisaster recovery含めて大変</p>

<p>Kinesisならシャードを100個立てれば終了</p>

<p>Grasslandというツールを開発中: データを分析して</p>

<h2>GrasslandでのKinesisの使いどころ</h2>

<p>1 クライアントからはKinesisにデータを直接投げる
2 ElasticBeansTalkはS3にキャシュを保存
3 S3はcacheの解析リクエストをKinesisに返す
4 Kinesis &ndash;> EBT に解析リクエストを投げる
5 S3 &ndash;> EBT はキャッシュを返す
6 EBT &ndash;> S3 解析結果を渡す</p>

<p>セキュリティを担保できる工夫はいろいろできる</p>

<p>Kinesisは世界のデータを集めるように作られている</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS Summit Tokyo 2014: AWS + Windows(C#)で構築する.NET最先端技術によるハイパフォーマンスウェブアプリケーション開発実践]]></title>
    <link href="http://chokkoyamada.github.io/blog/2014/07/18/aws-summit-tokyo-2014-grani-windows/"/>
    <updated>2014-07-18T13:19:00+09:00</updated>
    <id>http://chokkoyamada.github.io/blog/2014/07/18/aws-summit-tokyo-2014-grani-windows</id>
    <content type="html"><![CDATA[<p>ソーシャルゲームデベロッパーのGraniの河合さんによるC#のウェブアプリケーションの解説です。</p>

<p>私はWindowsでアプリケーションを書いたことも運用したこともないのですが、かなり昔にMS-DOSでシステム管理は少しやったことがあるので、Microsoftの技術には愛着を持っています。</p>

<p>一番驚いたのは、C#の機能をを活かして処理をほぼ非同期で行っているというところでした。
PHPなどでは絶対無理な処理なので、これはすごいなと。
私のプロダクトでやるならScalaになるのでしょうが、ここまでの境地に達するのはよほど言語に精通していないといけない気がします。</p>

<p>ソーシャルゲームのレスポンスで全て100msecを切るように返すことの大変さは分かっているつもりなので、レベルの違いを感じました。
Redisのパイプライン処理いいですね。</p>

<p>とにかくクールだなと思いました。ひたすら感心。</p>

<p>門外漢のC#やWindowsの話をあえて聞いてみてよかったです。</p>

<ul>
<li>AWS + C#によるウェブソーシャルゲーム</li>
<li>C# 50%, AWS 20%, その他 30%ぐらいの話</li>
<li>AWSだからといってC#を使うのに特別なことはない</li>
</ul>


<h2>using CSharp;</h2>

<ul>
<li>C#の開発にとことんこだわっている</li>
<li>もとはPHPだったが開発効率に不満を感じた</li>
<li>半年後にC#に全面移行</li>
<li><p>AWSを使っていたのでそのまま移行した</p></li>
<li><p>クライアントサイドからサーバーサイドまで全てをC#で！</p></li>
</ul>


<p>AWS + C#は全く問題ない。</p>

<ul>
<li>100 server</li>
<li>10000 req/sec</li>
<li>1000000000pv/day</li>
</ul>


<p>IIS8 + RDS + Redis(r3.2xlarge)20台</p>

<h2>なぜRDBMSを選択するのか</h2>

<ul>
<li>NoSQLでいい？けれど複雑化をおさえたい</li>
<li>RDBMSは単純なクエリなら1msec以下で返すがDynamoDBは・・・</li>
<li>周辺ツールが充実</li>
<li>スケールアップや分割で対応できるなら、スケールアウトに固執することもない</li>
</ul>


<h3>何よりRDSが良い</h3>

<p>手放し管理、Restore To Point in Timeには救われた</p>

<ul>
<li>SQL Server vs MySQL

<ul>
<li>C#としてはSQL Serverのほうが相性はいい</li>
<li>しかしRDSとしてみるとSQL Serverは貧弱</li>
<li>今はあるがMulti-AZも無かったし、ProvisionedIOPSの限界値も低かった</li>
<li>結果、MySQLを使っている</li>
</ul>
</li>
</ul>


<p>r3.8xlargeが最強</p>

<p>垂直分割 vs 水平分割</p>

<p>機能単位の垂直分割を採用</p>

<p>水平分割は避ける: 記述可能なクエリに大きな制限がかかってしまう アドホックなクエリでの集計が不可能になる
ほとんどのアプリケーションは、水平分割をする必要はないのでは？</p>

<p>DB管理はGUIで
HeidiSQLがおすすめ　phpMyAdminやCUIは積極的にdisりたい</p>

<p>アプリケーションからはMasterのみ参照
いまどきのRDSは十分パワフルなので、Slaveに分散するのは無用な複雑さを生むだけ
レプリケーション遅延は起きるので。</p>

<p>同一AZに配置する
AZを越えると普段0.5msec程度のクエリが2msec程度になる</p>

<p>C#的な活用としては、コネクションを型で分けるとコンパイルでチェックできる
生のSQLを書いている</p>

<p>テーブル定義から自動生成
テーブルと1:1でひもづいたクラスがある
T4テンプレート + ADO.net</p>

<p>半自動生成くらいの位置づけ</p>

<ul>
<li><p>全階層でのキャッシュの徹底</p>

<ul>
<li>データベース</li>
<li>Memcache-Redis</li>
<li>Static変数: アイテム名など不変の情報はキャッシュ　</li>
<li>リクエスト単位</li>
</ul>
</li>
</ul>


<p>アプリケーションコードによるJOIN</p>

<p>LINQ to Objectsで簡単。SQLよりもずっと楽</p>

<h2>Redis</h2>

<p>RDSの不得意な部分を補える
高パフォーマンスなのでMemcached代わりのキャッシュ用途に
用途ごとのグループ分けと単純分散</p>

<p>Master Slave構成をとっている
dump出力時にCPUをくうので</p>

<p>ElastiCache Redisは試したときにパフォーマンスがでなかったので使っていない</p>

<p>Protocol Buffersを使っている
高速・省サイズなので</p>

<h2>Performance + Async</h2>

<p>100msecを切るように作っている
C# 5.0を活用</p>

<p>言語構文レベルでサポートされる非同期
同期と同じように非同期が書ける Graniではコード全体が非同期で書かれている</p>

<p>Redisならパイプライン化可能
全てが非同期で自動でパイプライン化される</p>

<p>性能と設計を両立できる</p>

<h2>Log for performance</h2>

<p>外部通信を全て記録する
アプリケーション側で送信前後をフックして記録
後述するGlimpseやSumo Logicで解析する
アプリ側でやると負荷がない</p>

<h3>Glimpse &ndash; 可視化</h3>

<p>全体の実行時間のほか、あらゆるメトリックスを常時可視化
<a href="http://getglimpse.com">http://getglimpse.com</a></p>

<p>Explainの常時表示　手動でExplainはやらない
Redis送受信の表示</p>

<h3>日常的開発</h3>

<p>Git + GitHub
Jenkins
DeployはValentia(自社ツール)</p>

<p>Sumo Logicでログ解析
New Relicでモニタリング</p>

<p>Semantic Logging + Redshift + Tableau
<a href="http://slab.codeplex.com">http://slab.codeplex.com</a></p>

<h2>まとめ</h2>

<ul>
<li>C# + AWSは現実界</li>
<li>構成は堅く、シンプルに</li>
<li>環境は常に最新に</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS Summit Tokyo 2014: Amazon CloudFrontを利用したサイト高速化およびセキュア配信]]></title>
    <link href="http://chokkoyamada.github.io/blog/2014/07/18/aws-summit-tokyo-2014-cloudfront/"/>
    <updated>2014-07-18T12:21:00+09:00</updated>
    <id>http://chokkoyamada.github.io/blog/2014/07/18/aws-summit-tokyo-2014-cloudfront</id>
    <content type="html"><![CDATA[<p>ソリューションアーキテクトの北迫さんによる、Cloudfrontの活用方法についてです。</p>

<p>普段RTBをやっているので、相手はユーザーのブラウザではなくてSSPやアドネットワークのAPIで、普段CDNを意識することはあまりありません。
バナー広告の画像やJavaScriptを置いているくらいです。
将来、動画配信とかするなら詳しくなっておきたいなと・・・</p>

<p>ただ現時点でも、劇的にコストや負荷を軽減する方法として、CDNとしてのCloudfrontにはまだ利用シーンがあるのではと思って考えています。</p>

<h2>サイト高速化</h2>

<p>DNSの仕組みを使って最適なエッジへ誘導
52 Edge locations</p>

<p>80:20の法則
ページ本体のレスポンスタイムは20%に過ぎず、それ以外の部分に80%の時間を使っている</p>

<p>ispとispの間のixのところから足が生える形でAWS edgeがある</p>

<h3>CDN Edge Locationの活用</h3>

<p>分散型CDNと集中型のCDN</p>

<p>分散型CDN: ユーザーに近いISPのところにEdgeがある キャッシュヒット率が低い傾向
集中型CDN: キャッシュ共有 Cloudfrontはこちら</p>

<p>CloudfrontのEdgeに可能な限りキャッシュさせることが重要</p>

<ul>
<li><p>静的コンテンツ 805
キャッシュTTLも可能な限り長く、クライアント側にもキャッシュさせる</p></li>
<li><p>動的コンテンツ 20%</p>

<ul>
<li>ページ共通のもの</li>
<li>パーソナライズされたもの

<ul>
<li>動的に生成されるが、ページ自体は一定期間共通 QueryStringを活用しているもの</li>
<li>HTTPヘッダによるページ切り替えしているもの</li>
<li>cache-conrtol headerとminimum ttlで調整</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>キャッシュしすぎるとEdgeを通すオーバーヘッドはないか？
last-modified/ETag ヘッダ
if-modified-since 更新なしなら304を返す</p>

<ul>
<li><p>ヒット率向上のための要素</p>

<ul>
<li>キャッシュ時間</li>
<li>URLの共通化</li>
<li>Etag/Last-modifiedヘッダの活用</li>
<li>Query Stringsパラメータの固定化</li>
<li>転送対象Header値の固定化</li>
</ul>
</li>
<li><p>Cloudfrontは完全一致でキャッシュを共有</p></li>
<li><p>キャッシュヒット状況はレスポンスヘッダで</p>

<ul>
<li>X-Cacheの値を見る</li>
<li>X-Amz-Cf-IdにIDが入る</li>
</ul>
</li>
</ul>


<p>キャッシュができない動的ページはプロキシとして利用</p>

<p>Dynamic Contens Acceleration</p>

<ul>
<li>Post/Put/Header/Cookie対応</li>
</ul>


<p>CloudfrontのEdgeを経由させても多くの動的ページが扱えるようになった</p>

<ul>
<li>Keep-Alive Connection:  3 way handshakeのカット</li>
</ul>


<p>CDN &ndash; Originの間でKeepAliveする
HTTPS通信ならSSL Terminationでより効果が大きい</p>

<ul>
<li>TCPスロースタート</li>
</ul>


<p>ネットワークの輻輳を回避するために少しつづパケットを増やしながら量を増やしていく仕組み</p>

<ul>
<li>DNS Lookupの高速化</li>
</ul>


<p>Route53を活用するとLookupが早くなる: Cloudfrontと同じロケーションにサーバーがある &ndash;> 近い
CloudfrontはAlternative Domain Name: レコードセットのTypeをCNAMEではなくAレコードのエイリアスを利用することでクエリ回数の削減が可能</p>

<p>Cloudfront behaviorの活用
正規表現でURL毎に異なるキャッシュポリシーを適用できる
オリジンサーバーのほうで設定をする必要がない　</p>

<h2>セキュア配信</h2>

<ul>
<li><p>Httpsサポート</p></li>
<li><p>GEO Restriction</p></li>
<li><p>Signed URL(署名付きURL)</p>

<ul>
<li>Cloudfront経由で配信するコンテンツに対して期間指定URLを生成することで、配信コンテンツを保護する機能</li>
<li>ポリシーを指定する</li>
<li>Origin Access Identity(OAI)を使うと特定のCloudfrontからしかアクセスできないようにできる。一般のサーバーから制限する場合はIPアドレスで(CloudfrontのIPは公開されている)</li>
<li>プレミアムコンテンツ配信, Streaming配信などに利用</li>
<li>有効時間の設定は、ダウンロードならサイズにかかわらず短時間でOK　ストリーミングは映像・音声の再生時間+αを設定</li>
<li>Behavior毎の設定、正規表現を利用してDistributionを分けなくても特定のコンテンツのみ保護可能</li>
<li>Cache-controlヘッダも同様に利用できる</li>
</ul>
</li>
</ul>


<h2>まとめ</h2>

<ul>
<li>インフラアプローチでの高速化施策</li>
<li>動的コンテンツにもうまく活用できる</li>
<li>セキュアなコンテンツ配信も容易かつ高速化が可能</li>
</ul>


<p>2014/9/9 AWS Cloud Strage &amp; DB Day
<a href="http://csd.awseventsjapan.com">http://csd.awseventsjapan.com</a></p>
]]></content>
  </entry>
  
</feed>
